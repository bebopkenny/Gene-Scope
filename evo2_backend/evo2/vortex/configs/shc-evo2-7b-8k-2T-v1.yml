model_name: shc-evo2-7b-8k-2T-v1

vocab_size: 512
hidden_size: 4096
num_filters: 4096
hcl_layer_idxs: [2,3,4,7,11,12,15,18,20,23,26,27,30]
hcm_layer_idxs: [1,6,10,14,17,22,25,29]
hcs_layer_idxs: [0,5,9,13,16,21,24,28]
attn_layer_idxs: [8,19,31]

hcm_filter_length: 128
hcs_filter_length: 7
num_layers: 32

# Length of the short, depthwise FIR applied to input projections
short_filter_length: 3 
num_attention_heads: 32
short_filter_bias: false # add bias to FIR
mlp_init_method: torch.nn.init.zeros_
mlp_output_init_method: torch.nn.init.zeros_
eps: 0.000001
state_size: 8
rotary_emb_base: 10000 
make_vocab_size_divisible_by: 8
inner_size_multiple_of: 16  # force GLU inner_size to be a multiple of
inner_mlp_size: 11008
log_intermediate_values: False
# Number of groups in GQA
proj_groups: 1
# Number of groups in grouped 
hyena_filter_groups: 1
# Split strategy for channels
column_split_hyena: True
column_split: True
# Legacy options for MP / PP inference
model_parallel_size: 1
pipe_parallel_size: 1
tie_embeddings: True
mha_out_proj_bias: True
hyena_out_proj_bias: True
qkv_proj_bias: False
max_seqlen: 32768
max_batch_size: 1
final_norm: True 
use_flash_attn: True
use_flash_rmsnorm: False
use_flash_depthwise: False
use_flashfft: False
use_laughing_hyena: False
inference_mode: True
tokenizer_type: CharLevelTokenizer 
prefill_style: fft
mlp_activation: gelu
print_activations: False
ground_truth_activations_path: None

# {
#   "pipe_parallel_size": 0,
# "model_parallel_size": 1,
# "make_vocab_size_divisible_by": 8,

# "num_layers": 32,
# "hidden_size": 4096,
# "num_groups_hyena": 4096,
# # "num_groups_hyena_medium": 320,
# # "num_groups_hyena_short": 320,
# # "num_groups_hyena_mlp": 320,
# "num_attention_heads": 32, 
# # "num_heads": 1024, 
# "operator-config": [ 
#   [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 2],
#   [["hyena"], 1], [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 1],  # 8
#   [["flash_v2"], 1],
#   [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 1],
#   [["hyena"], 1], [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 1],
#   [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 1],  # 10
#   [["flash_v2"], 1],
#   [["hyena"], 1], [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 1],
#   [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 1],
#   [["hyena"], 1], [["hyena_short_conv"], 1], [["hyena_medium_conv"], 1], [["hyena"], 1],  #  11
#   [["flash_v2"], 1],
#   ],
# "seq_length": 8192,
# "max_position_embeddings": 8192,
# "hyena_medium_conv_len": 128,  # default is null
# "log_attn_norms": false,
# "pos_emb": "rotary",
# # "rotary_emb_base": 1000000,
# "rotary_pct": 1,
# "prenorm": true,
# "postnorm": false,
# "pre_mlp_norm": true,  
# "outer_mlp_norm": false,
# "no_weight_tying": false,
# "gpt_j_residual": false,
# "normalize_hyena_filters": false,
# "short-conv-L": 3, 
# "hyena_filter_fast_decay": 0.3,
# "hyena_filter_slow_decay": 1.2,
# "hyena_filter_w": 14, 
# "hyena_filter_cls": "implicit_complex_modal",
# # "hyena_medium_filter_cls": "explicit_single_decay",  
# # "explicit_filter_decay_preset": "weak",
# # "modal_residue_factors": 5,
# # "modal_pole_factors": 1,
# "hyena_filter_order": 16,
# "hyena_filter_wd": 0.,
# "use_fast_heads": false,
# "use_slow_heads": false,
# "use-hyena-filter": true,
# "output_layer_parallelism": "column",
# "bias_dropout_fusion": false,
# "norm": "rmsnorm",
# "rms_norm_epsilon": 1.0e-6,
# "identity_mlp": false,
#   "mlp_type": "llama",
#   "scaled-upper-triang-masked-softmax-fusion": true,
#   "bias-gelu-fusion": false,
#   "init_method": "small_init",
#   "output_layer_init_method": "wang_init",
#   "optimizer": {
#     "type": "Adam",
#     "params": {
#       "lr": 0.0003,
#       "betas": [0.9, 0.95],
#       "eps": 1.0e-8,
#     }
#   },
#   "min_lr": 0.00003,
#   "zero_optimization": {
#   "stage": 1,
#   "allgather_partitions": True,
#   "allgather_bucket_size": 500000000,
#   "overlap_comm": True,
#   "reduce_scatter": True,
#   "reduce_bucket_size": 500000000,
#   "contiguous_gradients": True,
#   "cpu_offload": false
# },
#   "train_micro_batch_size_per_gpu": 2,
#   "gradient_accumulation_steps": 1,
#   "data-impl": "mmap",
#   "checkpoint-activations": true,
#   "checkpoint-num-layers": 4,
#   "partition-activations": true,
#   "synchronize-each-layer": false,
#   "gradient_clipping": 1.0,
#   "weight-decay": 0.1,
#   "hidden-dropout": 0.0,
#   "attention-dropout": 0.0,
#   "precision": "bfloat16",
#   "bf16": {
#   "enabled": true
#   },
#   "train-iters": 500000,
#   "lr-decay-iters": 500000,
#   "distributed-backend": "nccl",
#   "lr-decay-style": "cosine",
#   "warmup": 0.005,
#   "checkpoint-factor": 2500,
# "extra_save_iters": [4],
#   "eval-interval": 200,
#   "eval-iters": 20,
#   "log-interval": 5,
#   "steps_per_print": 5,
#   "keep-last-n-checkpoints": 100,
#   "wall_clock_breakdown": false,
#   "save": "/checkpoint/hielab/etnguyen/evo2/7b_13h_8m_8s_3a_cascade15", # change this
#   "tokenizer_type": CharLevelTokenizer,   
# #  "iteration": 200,
# #  "use_checkpoint_lr_scheduler": True,
#   "use_fp8_linears": true,
#   "master_port": 29502,
# #  "include": "localhost@localhost:0",
# #  "num_gpus": 1,
# #  "bidirectional": True,
# "make_gated_mlp_multiple_of": 128,
# "materialize_attn_mask": false,  # default false, to save memory
# "fast_conv_proj": true,   
# "hyena_short_conv_len": 7,
# "use_wandb": true,  
# "to_upper": "weighted",
# "lowercase_loss_reweighting": 0.1,  
# "hyena_mlp_len": 7,  

# # multinode
# "launcher": "pdsh",
# "hostfile": "./hostfile_eric",
# "deepspeed_slurm": false,
# }